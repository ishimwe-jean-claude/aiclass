{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOfqk0zqDK6SUZeAM2/G4Rq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ishimwe-jean-claude/aiclass/blob/main/full_code_rag.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aMqBQqdKKJO2"
      },
      "outputs": [],
      "source": [
        "!pip install langchain-openai -q\n",
        "!pip install langchain-community -q\n",
        "!pip install langchain-experimental -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4iZ20v410APc"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "class ZhipuAI_embeddings:\n",
        "    def __init__(self, model_name: str = 'embeddings-3'):\n",
        "        self.model_name = model_name\n",
        "        self.base_url = \"https://open.bigmodel.cn/api/paas/v4\"\n",
        "        self.embedding = self._init_model()\n",
        "    def _init_model(self) -> OpenAIEmbeddings:\n",
        "        return OpenAIEmbeddings(\n",
        "            model=self.model_name,\n",
        "            base_url=self.base_url,\n",
        "            api_key=userdata.get(\"ishimwe_k\")\n",
        "        )\n",
        "embeddings =ZhipuAI_embeddings().embedding"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "client  = ChatOpenAI(\n",
        "    base_url =\"https://open.bigmodel.cn/api/paas/v4/\",\n",
        "    api_key = userdata.get(\"ishimwe_k\"),\n",
        "    model = \"glm-4.5\"\n",
        ")"
      ],
      "metadata": {
        "id": "mZ51WU2_UJC-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "client.invoke(\"hello\").cotent"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "R_WTJllH4nm2",
        "outputId": "e82ddd59-8a94-4f43-ff18-07173ba9fd01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Hello! How can I assist you today? 😊', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 13, 'prompt_tokens': 8, 'total_tokens': 21, 'completion_tokens_details': None, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'glm-4.5', 'system_fingerprint': None, 'id': '20250912174909ce8b9ff152de445e', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--409ade20-de86-4a5b-b226-600d75eb6e16-0', usage_metadata={'input_tokens': 8, 'output_tokens': 13, 'total_tokens': 21, 'input_token_details': {'cache_read': 0}, 'output_token_details': {}})"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.documents import Document\n",
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "from langchain_community.document_loaders.text import TextLoader\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "def doc_parsing(file_path) -> list[Document]:\n",
        "    if file_path.endswith(\"pdf\"):\n",
        "        doc = PyPDFLoader(file_path=file_path)\n",
        "        doc = doc.load()\n",
        "        semantic_splitter = SemanticChunker(\n",
        "            embeddings=embeddings,\n",
        "            breakpoint_threshold_type=\"percentile\",\n",
        "            breakpoint_threshold_amount=95\n",
        "        )\n",
        "        full_text = doc[0].page_content if doc else \"\"\n",
        "        if not full_text:\n",
        "            return []\n",
        "        raw_chunks = semantic_splitter.split_text(full_text)\n",
        "        print(f\" the number of chucks :{len(raw_chunks)}\")\n",
        "        docs = [Document(page_content=chunk, metadata=doc[0].metadata) for chunk in raw_chunks]\n",
        "        return docs\n",
        "    elif file_path.endswith(\".txt\"):\n",
        "        doc = TextLoader(file_path=file_path)\n",
        "        doc =doc.load()\n",
        "        semantic_splitter = SemanticChunker(\n",
        "            embeddings=embeddings,\n",
        "            breakpoint_threshold_type=\"percentile\",\n",
        "            breakpoint_threshold_amount=95\n",
        "        )\n",
        "        full_text = doc[0].page_content if doc else \"\"\n",
        "        if not full_text:\n",
        "            return []\n",
        "        raw_chunks = semantic_splitter.split_text(full_text)\n",
        "        docs = [Document(page_content=chunk, metadata=doc[0].metadata) for chunk in raw_chunks]\n",
        "        return docs\n",
        "    else:\n",
        "       return []"
      ],
      "metadata": {
        "id": "HzoSneZt0OtJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.documents import Document\n",
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "from langchain_community.document_loaders.text import TextLoader\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "def doc_parsing(file_path) -> list[Document]:\n",
        "    if file_path.endswith(\"pdf\"):\n",
        "        doc = PyPDFLoader(file_path=file_path)\n",
        "        doc = doc.load()\n",
        "        semantic_splitter = SemanticChunker(\n",
        "            embeddings=embeddings,\n",
        "            breakpoint_threshold_type=\"percentile\",\n",
        "            breakpoint_threshold_amount=95\n",
        "        )\n",
        "        full_text = doc[0].page_content if doc else \"\"\n",
        "        if not full_text:\n",
        "            return []\n",
        "        raw_chunks = semantic_splitter.split_text(full_text)\n",
        "        print(f\" the number of chucks :{len(raw_chunks)}\")\n",
        "        docs = [Document(page_content=chunk, metadata=doc[0].metadata) for chunk in raw_chunks]\n",
        "        return docs\n",
        "    elif file_path.endswith(\".txt\"):\n",
        "        doc = TextLoader(file_path=file_path)\n",
        "        doc =doc.load()\n",
        "        semantic_splitter = SemanticChunker(\n",
        "            embeddings=embeddings,\n",
        "            breakpoint_threshold_type=\"percentile\",\n",
        "            breakpoint_threshold_amount=95\n",
        "        )\n",
        "        full_text = doc[0].page_content if doc else \"\"\n",
        "        if not full_text:\n",
        "            return []\n",
        "        raw_chunks = semantic_splitter.split_text(full_text)\n",
        "        docs = [Document(page_content=chunk, metadata=doc[0].metadata) for chunk in raw_chunks]\n",
        "        return docs\n",
        "    else:\n",
        "       return []"
      ],
      "metadata": {
        "id": "aKObR_N1UYpc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s_template = \"\"\"\n",
        "Your name is AICLASS assistant a smart, yet sassy assistant working at takenolab, your have a better knowledge of takenolab operations,\n",
        "your task is to be supportive, provide proper guidance to students that are having troubles with the course content,\n",
        "you have to answer them direct and precise, if you dont have any advice to them dont generate any respose kindly\n",
        "tell them so.\n",
        "When you receive:\n",
        "• question: the student’s problem\n",
        "• context: optionally, the content of any uploaded documents\n",
        "\n",
        "If `context` is non-empty, you **must** use it to inform your answer. If you don’t have enough information\n",
        "from the question and context combined, tell the student you can’t help further.\n",
        "\n",
        "student problem:\n",
        "{question}\n",
        "\n",
        "uploaded document context (if any):\n",
        "{context}\n",
        "\n",
        "your smart advice or solution:\n",
        "\n",
        "\"\"\"\n",
        "new_template = \"\"\"\n",
        "Your name is AICLASS assistant, a smart, yet sassy assistant working at takenolab. You have a deep knowledge of takenolab operations.\n",
        "Your task is to be supportive, provide proper guidance to students who are having trouble with the course content.\n",
        "You must answer them directly and precisely. If you don't have any advice for them, kindly tell them so and do not generate any other response.\n",
        "\n",
        "When you receive:\n",
        "• chat_history: the previous turns of the conversation (if any)\n",
        "• question: the student’s current problem\n",
        "• context: optionally, the content of any uploaded documents relevant to the current question\n",
        "\n",
        "If `context` is non-empty, you **must** use it to inform your answer. If you don’t have enough information\n",
        "from the question and context combined, tell the student you can’t help further.\n",
        "If `chat_history` is provided, use it to understand the full context of the current question.\n",
        "\n",
        "<chat_history>\n",
        "{chat_history}\n",
        "</chat_history>\n",
        "\n",
        "student problem:\n",
        "{question}\n",
        "\n",
        "uploaded document context (if any):\n",
        "{context}\n",
        "\n",
        "your smart advice or solution:\n",
        "\n",
        "\"\"\"\n",
        "SUB_QUERY_TEMPLATE = \"\"\"\n",
        "You are a helpful assistant that generates multiple search queries based on a single input query.\n",
        "Generate {num_queries} diverse search queries related to the user's question, which can be used to retrieve relevant documents.\n",
        "The queries should be concise and cover different aspects or angles of the original question.\n",
        "\n",
        "Original Question: {question}\n",
        "\n",
        "Generated Queries:\n",
        "-\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "qeGvRozg0yD9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.documents import Document\n",
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "from langchain_community.document_loaders.text import TextLoader\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "def doc_parsing(file_path) -> list[Document]:\n",
        "    if file_path.endswith(\"pdf\"):\n",
        "        doc = PyPDFLoader(file_path=file_path)\n",
        "        doc = doc.load()\n",
        "        semantic_splitter = SemanticChunker(\n",
        "            embeddings=embeddings,\n",
        "            breakpoint_threshold_type=\"percentile\",\n",
        "            breakpoint_threshold_amount=95\n",
        "        )\n",
        "        full_text = doc[0].page_content if doc else \"\"\n",
        "        if not full_text:\n",
        "            return []\n",
        "        raw_chunks = semantic_splitter.split_text(full_text)\n",
        "        print(f\" the number of chucks :{len(raw_chunks)}\")\n",
        "        docs = [Document(page_content=chunk, metadata=doc[0].metadata) for chunk in raw_chunks]\n",
        "        return docs\n",
        "    elif file_path.endswith(\".txt\"):\n",
        "        doc = TextLoader(file_path=file_path)\n",
        "        doc =doc.load()\n",
        "        semantic_splitter = SemanticChunker(\n",
        "            embeddings=embeddings,\n",
        "            breakpoint_threshold_type=\"percentile\",\n",
        "            breakpoint_threshold_amount=95\n",
        "        )\n",
        "        full_text = doc[0].page_content if doc else \"\"\n",
        "        if not full_text:\n",
        "            return []\n",
        "        raw_chunks = semantic_splitter.split_text(full_text)\n",
        "        docs = [Document(page_content=chunk, metadata=doc[0].metadata) for chunk in raw_chunks]\n",
        "        return docs\n",
        "    else:\n",
        "       return []"
      ],
      "metadata": {
        "id": "HSnNXMxYUckS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s_template = \"\"\"\n",
        "Your name is AICLASS assistant a smart, yet sassy assistant working at takenolab, your have a better knowledge of takenolab operations,\n",
        "your task is to be supportive, provide proper guidance to students that are having troubles with the course content,\n",
        "you have to answer them direct and precise, if you dont have any advice to them dont generate any respose kindly\n",
        "tell them so.\n",
        "When you receive:\n",
        "• question: the student’s problem\n",
        "• context: optionally, the content of any uploaded documents\n",
        "\n",
        "If `context` is non-empty, you **must** use it to inform your answer. If you don’t have enough information\n",
        "from the question and context combined, tell the student you can’t help further.\n",
        "\n",
        "student problem:\n",
        "{question}\n",
        "\n",
        "uploaded document context (if any):\n",
        "{context}\n",
        "\n",
        "your smart advice or solution:\n",
        "\n",
        "\"\"\"\n",
        "new_template = \"\"\"\n",
        "Your name is AICLASS assistant, a smart, yet sassy assistant working at takenolab. You have a deep knowledge of takenolab operations.\n",
        "Your task is to be supportive, provide proper guidance to students who are having trouble with the course content.\n",
        "You must answer them directly and precisely. If you don't have any advice for them, kindly tell them so and do not generate any other response.\n",
        "\n",
        "When you receive:\n",
        "• chat_history: the previous turns of the conversation (if any)\n",
        "• question: the student’s current problem\n",
        "• context: optionally, the content of any uploaded documents relevant to the current question\n",
        "\n",
        "If `context` is non-empty, you **must** use it to inform your answer. If you don’t have enough information\n",
        "from the question and context combined, tell the student you can’t help further.\n",
        "If `chat_history` is provided, use it to understand the full context of the current question.\n",
        "\n",
        "<chat_history>\n",
        "{chat_history}\n",
        "</chat_history>\n",
        "\n",
        "student problem:\n",
        "{question}\n",
        "\n",
        "uploaded document context (if any):\n",
        "{context}\n",
        "\n",
        "your smart advice or solution:\n",
        "\n",
        "\"\"\"\n",
        "SUB_QUERY_TEMPLATE = \"\"\"\n",
        "You are a helpful assistant that generates multiple search queries based on a single input query.\n",
        "Generate {num_queries} diverse search queries related to the user's question, which can be used to retrieve relevant documents.\n",
        "The queries should be concise and cover different aspects or angles of the original question.\n",
        "\n",
        "Original Question: {question}\n",
        "\n",
        "Generated Queries:\n",
        "-\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "XaN93mH5UckT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.tools import tool\n",
        "\n",
        "@tool\n",
        "def define_term(term:str):\n",
        "  \"\"\"  Returns a one-sentence definition of a given AI term\n",
        "     (e.g., “Transformer”, “Embedding”).\"\"\"\n",
        "  return f\"The definition of {term} is ...\"\n",
        "\n",
        "@tool\n",
        "def summarize_notes(text: str):\n",
        "  \"\"\" Takes student notes (a paragraph) and returns a\n",
        "  concise summary (2–3 sentences). \"\"\"\n",
        "  return f\"The summary of the notes is ...\"\n",
        "\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "model = client.bind_tools([define_term, summarize_notes])\n",
        "prompt_template = PromptTemplate.from_template(template=s_template)\n",
        "new_template = prompt_template | model\n",
        "\n",
        "\n",
        "response = define_term.invoke(\"natural language processing\")\n",
        "print(response)\n"
      ],
      "metadata": {
        "id": "wmjFg0enUj3p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "import gradio as gr\n",
        "from langchain.chains import LLMChain\n",
        "from langchain_core.documents import Document\n",
        "from typing import TypedDict, List\n",
        "from langchain_core.vectorstores import InMemoryVectorStore\n",
        "from langchain_core.messages import AIMessage, HumanMessage, BaseMessage\n",
        "from typing import Optional,Tuple, Dict\n",
        "\n",
        "vector_store = InMemoryVectorStore(embeddings)\n",
        "def call_assistant(question:str=\"\", context_docs:List[Optional[Document]]=None,chat_history: List[Tuple[str, str]]=None):\n",
        "    prompt_template = PromptTemplate.from_template(template=new_template)\n",
        "    chain = prompt_template | client\n",
        "    history_str = \"\"\n",
        "    if chat_history:\n",
        "        for human_msg, ai_msg in chat_history:\n",
        "            history_str += f\"User: {human_msg}\\n Assistant: {ai_msg}\\n\"\n",
        "    context_text =\"\"\n",
        "    if context_docs:\n",
        "         context_text = \"\\n\\n\".join(d.page_content for d in context_docs)\n",
        "    response = chain.invoke({\n",
        "        \"question\": question,\n",
        "        \"context\": context_text,\n",
        "        \"chat_history\": history_str\n",
        "    })\n",
        "    return response.content\n",
        "def generate_sub_queries(original_question: str, num_queries: int = 3) -> List[str]:\n",
        "    sub_query_prompt = PromptTemplate.from_template(template=SUB_QUERY_TEMPLATE)\n",
        "    sub_query_chain = sub_query_prompt | client.client\n",
        "\n",
        "    response = sub_query_chain.invoke({\n",
        "        \"question\": original_question,\n",
        "        \"num_queries\": num_queries\n",
        "    })\n",
        "\n",
        "    queries = [q.strip() for q in response.content.split('-') if q.strip()]\n",
        "    print(f\"Generated sub-queries: {queries}\")\n",
        "    return queries\n",
        "def retrieve_and_answer_with_history(question: str, chat_history: List[Dict])->str:\n",
        "    # retrieved = vector_store.similarity_search(question)\n",
        "    formatted_chat_history_for_llm = []\n",
        "    for msg in chat_history:\n",
        "        if msg[\"role\"] == \"user\":\n",
        "            current_user_msg = msg[\"content\"]\n",
        "        elif msg[\"role\"] == \"assistant\":\n",
        "            formatted_chat_history_for_llm.append((current_user_msg, msg[\"content\"]))\n",
        "            current_user_msg = None\n",
        "    if len(vector_store.store.items()) >= 0:\n",
        "        transformed_queries = generate_sub_queries(question, num_queries=3)\n",
        "        all_retrieved_docs = []\n",
        "        seen_doc_contents = set()\n",
        "        for query in transformed_queries:\n",
        "            retrieved_for_query = vector_store.similarity_search(query)\n",
        "            for doc in retrieved_for_query:\n",
        "                if doc.page_content not in seen_doc_contents:\n",
        "                    all_retrieved_docs.append(doc)\n",
        "                    seen_doc_contents.add(doc.page_content)\n",
        "        ai_response = call_assistant(question, context_docs=all_retrieved_docs, chat_history=formatted_chat_history_for_llm)\n",
        "        return ai_response\n",
        "    ai_response = call_assistant(question,chat_history=formatted_chat_history_for_llm)\n",
        "    return ai_response\n",
        "def doc_loader(file_path):\n",
        "    docs = doc_parsing(file_path)\n",
        "    if not docs:\n",
        "        return \"No content found or processed in the document.\"\n",
        "    _ = vector_store.add_documents(documents=docs)\n",
        "    return docs[0].page_content[:200] + \"...\"\n",
        "\n",
        "def interface():\n",
        "    iface = gr.ChatInterface(\n",
        "        fn=retrieve_and_answer_with_history,\n",
        "        chatbot=gr.Chatbot(height=200, type='messages', label=\"Assistant\"),\n",
        "        textbox=gr.Textbox(lines=2,submit_btn=True ),\n",
        "        title=\"Takenolab AIClass Assistant (Conversational RAG)\",\n",
        "        type='messages',\n",
        "        description=\"Ask a question about your course content and get smart advice, supporting multi-turn conversations.\",\n",
        "    )\n",
        "    docs_interface = gr.Interface(\n",
        "        fn=doc_loader,\n",
        "        inputs=gr.File(label=\"Choose a file to upload\",\n",
        "                       type='filepath',\n",
        "                       file_count='single',\n",
        "                       show_label=True\n",
        "                       ),\n",
        "        description=\"Upload a document to run retrieval‐augmented generation.\",\n",
        "        outputs=gr.TextArea()\n",
        "    )\n",
        "    table = gr.TabbedInterface(\n",
        "        [iface,docs_interface],\n",
        "        tab_names= ['Chat', \"Upload File for RAG\"],\n",
        "        title=\"LLM, RAG AND PROMPTS, Text Generation\"\n",
        "    )\n",
        "    table.launch(debug=True, server_port=3000)\n"
      ],
      "metadata": {
        "id": "T6RcUBZQ0jSb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "interface()"
      ],
      "metadata": {
        "id": "NdBgywia3X2m",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "733300af-d57d-4de7-fb93-eee6a27b400d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://7c065424015ee43b7a.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://7c065424015ee43b7a.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/gradio/blocks.py\u001b[0m in \u001b[0;36mblock_thread\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3157\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3158\u001b[0;31m                 \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3159\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4121580267.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0minterface\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-2813711030.py\u001b[0m in \u001b[0;36minterface\u001b[0;34m()\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mtitle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"LLM, RAG AND PROMPTS, Text Generation\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     )\n\u001b[0;32m---> 93\u001b[0;31m     \u001b[0mtable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlaunch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mserver_port\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/gradio/blocks.py\u001b[0m in \u001b[0;36mlaunch\u001b[0;34m(self, inline, inbrowser, share, debug, max_threads, auth, auth_message, prevent_thread_lock, show_error, server_name, server_port, height, width, favicon_path, ssl_keyfile, ssl_certfile, ssl_keyfile_password, ssl_verify, quiet, show_api, allowed_paths, blocked_paths, root_path, app_kwargs, state_session_capacity, share_server_address, share_server_protocol, share_server_tls_certificate, auth_dependency, max_file_size, enable_monitoring, strict_cors, node_server_name, node_port, ssr_mode, pwa, mcp_server, _frontend, i18n)\u001b[0m\n\u001b[1;32m   3053\u001b[0m             )\n\u001b[1;32m   3054\u001b[0m         ):\n\u001b[0;32m-> 3055\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblock_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3056\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3057\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mTupleNoPrint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserver_app\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocal_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshare_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/gradio/blocks.py\u001b[0m in \u001b[0;36mblock_thread\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3160\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Keyboard interruption in main thread... closing server.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3161\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserver\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3162\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3163\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mtunnel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mCURRENT_TUNNELS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3164\u001b[0m                 \u001b[0mtunnel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/gradio/http_server.py\u001b[0m in \u001b[0;36mclose\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwatch_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/threading.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1151\u001b[0m             \u001b[0;31m# the behavior of a negative timeout isn't documented, but\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1152\u001b[0m             \u001b[0;31m# historically .join(timeout=x) for x<0 has acted as if timeout=0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1153\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_for_tstate_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1155\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_wait_for_tstate_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/threading.py\u001b[0m in \u001b[0;36m_wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1169\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1170\u001b[0m                 \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1171\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}